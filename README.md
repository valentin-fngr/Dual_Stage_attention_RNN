# A Transformer-Based Framework for Multivariate Time Series Representation Learning

## Overview

This repository contains an implementation of a **Transformer-based Framework** designed for **Multivariate Time Series Representation Learning**. This framework leverages the power of Transformer models to effectively capture complex patterns and dependencies in multivariate time series data.

## Key Features

- **Transformer Architecture**: Utilizes self-attention mechanisms to capture long-range dependencies and complex relationships within the data.
- **Multivariate Time Series Processing**: Designed to handle and learn from multiple time series variables simultaneously.
- **Enhanced Representation Learning**: Improves the quality of learned representations, leading to better performance on downstream tasks such as forecasting, classification, and anomaly detection.

## Prerequisites

- Python 3.x

## Installation

1. Clone the repository:
   ```bash
   git clone https://github.com/valentin-fngr/Dual_Stage_attention_RNN.git

2. Install the required packages: 
    ```bash 
    pip install -r requirements.txt

## Usage

The train.py script accepts the following command-line arguments:

    --config: Path to the configuration file.
    --checkpoint: Directory to save checkpoints.
    --finetune: Path to a checkpoint to fine-tune from.
    --seed: Random seed for reproducibility (default: 0).
